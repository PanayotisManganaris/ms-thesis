\chapter{MULTI-FIDELITY MACHINE LEARNING FOR PEROVSKITE BAND GAP PREDICTIONS}
\label{sec:orgde7d406}

The fidelity hierarchy in the sample climbs from DFT simulations performed using the basic PBE GGA functional, to results obtained from physical experiments aggregated in literature.
\autocite{almora-2020-devic-perfor,kim-2014-cdses-nanow,swanson-2017-co-sublim}
Low fidelity data makes up the majority of the sample and serves as the foundation for interpolation.
However, it does not accuracy reproduce the experimental measurements.
My work leverages the data covered in chapters 1 and 2 to predict the band gap of arbitrary perovskite compositions at experimental actuary with little anticipated error.

To do this, a set of interpretable descriptors of each perovskite are used.
This takes the form of a 14-dimensional vector containing the atomic fractions of each of the 14 constituent species within the specified perovskite formula.
This vector is a sufficient descriptor of a perovskite and has served decent predictions.
\autocite{mannodi-kanakkithodi-2022-data-driven}
To improve regression I examine an addition 36 additional predictors derived from linear combinations of compositions and elemental properties obtained from the trusted Mendeleev databases.
\autocite{mentel-2014}

\section{partitioning categorical data}
\label{sec:orgfa8b6f4}
\section{Model Optimization}
\label{sec:orgd11793f}
The rigorous hyper-parameter Optimization (HPO) of any feature engineering and modeling pipeline is a problem discussed extensively in the literature.
HPO approaches can be broadly separated into exhaustive and efficient optimization strategies.
\autocite{yang-2020-hyper-optim}
We use a two-stage procedure for selecting the best model parameters.

The first stage is an exhaustive grid-search over diversely sampled parameter space.
Each combination of parameters instantiates a model which is then fit to each of a set of stratified training subsets generated by a K=3 K-fold split cross-validation strategy.
Every fitted model is subsequently tested against the cross-validation test sets and a suite of regression scoring metrics are applied to each member category simultaneously using a custom SciKit-learn score adapter\footnote{\url{https://github.com/PanayotisManganaris/yogi}\label{org1cc6293}}.
The grid search is then narrowed to a high performance quadrant of the search space by the model evaluator based on recommendations made by a simple entropy minimization algorithm\textsuperscript{\ref{org1cc6293}}.

The recommended grid quickly eliminates under-performing settings based on the sample probability of a setting appearing in a set of finalists according to the scoring rankings.
The selection score is additionally influenced by a weighted sum of the scoring ranks allowing for considerably tuning the selection criterion.
 For best results, a few different grid spaces should be explored to corroborate eliminations.

After the recommendation is made, the granularity of the grid is increased in the remaining ambiguous parameters and the process is repeated.
In general, no more than 2 or 3 exhaustive searches are needed over a given set of grids.
Past this point, continuously variable hyper parameters can be individually optimized by plotting validation curves.

\section{Methods}
\label{sec:org0b61df4}
\subsection{Featurization of Chemistries}
\label{sec:orge40c1c6}
For \(\alpha\) total A-site constituents represented in the whole database, \(\beta\) total B-site constituents, and \(\gamma\) total X-site constituents, we provide a Python tool\footnote{\url{https://github.com/PanayotisManganaris/cmcl}\label{org5c87981}} which robustly coverts the composition string of each data point into a \(\alpha + \beta + \gamma\) dimensional composition vector.
In the case of our total dataset description \(\alpha + \beta + \gamma = 14\).
\autocite{yang-2022-high-throug}
In a subset of the data, the chemical vector (listing \ref{lst:cdf}) is produced using cmcl (listing \ref{lst:cmcl}).

\begin{ZZlisting}
  \caption{\label{lst:cmcl} An example of the cmcl "ft" feature accessor}
  \begin{CenteredBox}
    \begin{lstlisting}[language=python]
import cmcl
Y = load_codomain_subset()
df = Y.Formula.to_frame().ft.comp()
df.index = Y.Formula
print(df)
    \end{lstlisting}
  \end{CenteredBox}
\end{ZZlisting}

\begin{ZZlisting}
  \caption{\label{lst:cdf} Data frame of composition vectors generated by cmcl}
  \begin{CenteredBox}
    \begin{lstlisting}
                    FA   Pb   Sn    I   MA   Br
Formula                                        
FAPb_0.7Sn_0.3I_3  1.0  0.7  0.3  3.0  NaN  NaN
MAPb(I0.9Br0.1)3   NaN  1.0  NaN  2.7  1.0  0.3
    \end{lstlisting}
  \end{CenteredBox}
\end{ZZlisting}

This is naturally a sparse, relatively high dimensional descriptor.
With any growth in the composition space it becomes sparser.
This descriptor has been shown to be effective for interpolating the properties of irregularly mixed large supercells.
\autocite{mannodi-kanakkithodi-2022-data-driven}
However, a spare descriptor is generally bad for extrapolative modeling.
\autocite{ghiringhelli-2015-big-data} 

When extrapolation is the aim, continuously distributed, unique, and linearly independent features are much more reliable.
\autocite{lux-2020-inter-spars} 

Our attempts to provide a domain with these characteristics results in the following raw feature space.

\begin{itemize}
\item 14 sparse composition vectors extracted from chemical formula using \texttt{cmcl}\textsuperscript{\ref{org5c87981}}
\item 36 dense site-averaged property space computed as a linear combination of composition vectors and measured elemental properties \autocite{mentel-2014}
\item 5 categorical dimensions one-hot-encoding level of theory.
\begin{itemize}
\item this provides the categorical axis for multi-task learning
\item see table \ref{tbl:LoTs}
\end{itemize}
\end{itemize}

\subsection{Machine Learning Algorithms and Parameter Optimization}
\label{sec:orgcad1d7a}
We train Random Forest Regression (RFR) and Gaussian Process Regression (GPR) models of band gap on the union of predictor features previously discussed.
The RFR is a flexible nonlinear model, the GPR a principled linear model.
Shapley Additive Explaination (SHAP) analysis of the models lends insight to the average physical impacts of 1) site-specific alloying and 2) using organic molecules in the Perovskite superstructure.
 Model development and feature extraction is performed using Python and SciKit-Learn v1.2.
\autocite{pedregosa-2011-scikit-learn} 

We are careful to maintain the diversity of mixing types and hybrid-organic/inorganic samples within each fidelity subset.
We expect this will help to ensure the models learn relationships between fidelities, not differences in alloy scheme or constituency distributions within each fidelity.

Each model architecture is rigorously optimized with regard to both 1) generality over the domains of Perovskite compositions and site-averaged constituent properties and 2) generality over the domain of alloy classifications.

In order to monitor for possible categorical biases effecting regressions, nine metrics are used to evaluate the performance of each model over all alloy types at every stage of the hyper-parameter optimization.
This is done simultaneously, only models that perform uniformly well on all alloy types are selected.

We expect perovskites of a given alloy class and of a given hybrid-organic/inorganic status will perform significantly differently with respect to a particular application compared to perovskites of a another class or status.
We attempt to make models that reasonably explain this high entropy mixing diversity by utilizing the cardinal mixing represented in our sample.

We do this by training each model using two test/train splits.
First, the optimal model parameters are chosen for their performance under a random split.
A minimum of 3-fold cross-validation is performed for every set of model parameters that is considered.

Finally, the optimized model's ability to extrapolate is
tested by training/testing on splits determined with a groupwise
K-fol] splitting strategy.

Two separate cross validation schemes are employed at each stage of the design process.
First, the sample set is shuffled once and split to mitigate the models tendency to fit on sample order, then, stratified K-folds are generated in manner consistent with the types of each sample.
The regressor is then trained on the subsets of each class.
Its ability to extrapolate is independently metered on each validation fold consisting of members of the other classes.

Second, the ability for a model trained on samples belonging to one class/status to extrapolate to samples of another class/status is tested as well.
The samples again are shuffled and split.
then the training set is separated using a grouping K-fold split strategy.

A final best model is instantiated using the overall best performing parameters.
These models are finally validated against the test sets originally split off from the sample in both their extrapolative ability and consistency 
This procedure in demonstrated in an online notebook by \textcite{manganaris-2022-mrs-comput} hosted on the Purdue nanoHUB.

\subsection{Feature Engineering}
\label{sec:org65b0d16}
There has been success in creating analytical expressions for perovskite properties, particularly lattice parameters.
\autocite{jiang-2006-predic-lattic} In an attempt to find an analytical predictor for band gap we employ the Sure Independence Screening and Sparsifying Operator (SISSO).
\autocite{ouyang-2018-sisso} 

SIS\footnote{\url{https://github.com/rouyang2017/SISSO}} is a powerful application of compressed sensing.
\autocite{ghiringhelli-2017-learn-physic}
The SIS operator is a potent dimensionality reduction technique.
It does not perform any mathematical decomposition but instead picks existent dimensions that begin to approximate an orthogonal basis.
It outperforms CUR decomposition by functioning effectively in extremely high rank vector spaces.
\autocite{ray-2021-various-dimen,hamm-2019-cur-decom}
This is accomplished by posing the decomposition as a compressed sensing problem in the correlation metric space.

It allows the program to effectively find candidates for a linearly independent basis in a vector space of immense size.
unlike legacy techniques, e.g.
LASSO, it does not suffer when features are correlated.
\autocite{tibshirani-1996-regres-shrin,gauraha-2018-introd-to-lasso}

This allows for it to be used in performing a brute force search of a super-space generated by combinatorial operations on the raw predictor variables.

The Sparsifying Operator finds members of the resulting basis set which correlate with the target co-domain.
it does this by creating a sparsified linear model, similar again to a LASSO.
This process produces an analytic model of the target property, which is easy to interpret and can even be constrained for consistent combination of dimension units.

Subsequent applications of the SIS operator to the residuals of this model are a clever interrogation of error yielding more orthogonal basis sets that can be incorporated into the model.
\autocite{mayo-1998-error-growt}

SISSO is run for our dataset on the same partitioning scheme used by the previous models via an SciKit-learn compliant \autocite{buitinck-2013-api} interface\footnote{\url{https://github.com/PanayotisManganaris/pysisso}} extensively modified from the original Matgenix\footnote{\url{https://github.com/Matgenix/pysisso}} code.
Additionally, the algorithm is informed of features units so that it is restricted to meaningful linear combinations.
SIS features complexity is restricted to a maximum of 3 operations primarily to encourage parsimonious descriptions.
The available operation set is outlined in table \ref{tbl:ops}.

\begin{table}[htbp]
\caption{\label{tbl:ops} operations for formation of combinatorial super-space}
\centering
\begin{tabular}{ll}
Binary & Unary\\[0pt]
\hline
addition & reciprocation\\[0pt]
subtraction & power 2\\[0pt]
multiplication & power 3\\[0pt]
division & natural logarithm\\[0pt]
 & exponentiation\\[0pt]
 & root 2\\[0pt]
\end{tabular}
\end{table}

\section{Results}
\label{sec:org92966da}
\subsection{Best Models on Raw Domain}
\label{sec:org8e8ef68}
 
\begin{figure}[htbp]
\centering
\includesvg[width=450pt]{./.ob-jupyter/dc863520dccc5f4cefbddf3272f3866b5d78ae59}
\caption{\label{fig:pairplots} model predictions vs true values at multiple fidelities}
\end{figure}

The optimized models are high performing (Table \ref{tbl:LoTscores}).
The RFR hyper-parameters are listed in the appendix (Table \ref{tbl:rfrHPO}).

The GPR model is tried with multiple kernels.
Ultimately, the best is a non stationary Matern kernel with \(\nu = \frac{3}{2}\).

\subsection{SISSO Model and SIS Engineered Features}
\label{sec:orge6aea5e}
The Sure Independence Screening and Sparsifying Operator (SISSO) is a specific combination of multiple data mining techniques chained together resulting in a symbolically expressed regression model.
\autocite{ouyang-2018-sisso,ghiringhelli-2017-learn-physic} 

The best SISSO model for band gap involving 3 SIS features (each composed of up to 4 basic features) has an unremarkable RMSE of 0.476 eV, barely outperforming an OLS regression on 55 dimensions (see Table \ref{tbl:LoTscores}).
It is expressed in equation \ref{eq:bgexp}.
Notably, while the units of the expression do not match the units of band gap as measured (target units are unknown to the algorithm), they are still energy units.
This is by design, as the combination of features was restricted so to only allow compatible units to be combined.
A separate training session without this restriction was attempted, but the resulting model's performance was worse.

\begin{align}
\label{eq:bgexp}
bg\si{\electronvolt} = 1.752393064 &((X;\mbox{electronegativity}*A;\mbox{heat of fusion})-\nonumber\\&(B;\mbox{electron affinity}+B;\mbox{ionization energy}))\nonumber\\+-0.5862929089 &((B;Sn-\mbox{HSE})+(\mbox{PBE}-X;\mbox{electronegativity}))\nonumber\\+1.063684923 &((A;\mbox{electronegativity}-B;Ca)*(B;\mbox{heat of vap}-X;\mbox{electron affinity}))\nonumber\\+4.657097107
\end{align}

\begin{table}[htbp]
\caption{\label{tbl:LoTscores} RMSE of models on raw domain calculated per LoT subset}
\centering
\begin{tabular}{lrrrrrr}
rmse scores & GPR & RFR & Linear OLS & SISSO & SIS + GPR & SIS + RFR\\[0pt]
\hline
total & 0.156214 & 0.124738 & 0.499558 & 0.474754 & 0.251881 & 0.187431\\[0pt]
EXP & 0.120475 & 0.154448 & 0.307186 & 0.330080 & 0.338949 & 0.235397\\[0pt]
PBE & 0.128211 & 0.101872 & 0.472430 & 0.395827 & 0.171640 & 0.134529\\[0pt]
HSE & 0.214920 & 0.152479 & 0.558077 & 0.519706 & 0.305443 & 0.208390\\[0pt]
HSE(SOC) & 0.156785 & 0.108867 & 0.535087 & 0.572644 & 0.272158 & 0.221007\\[0pt]
HSE-PBE(SOC) & 0.130696 & 0.133027 & 0.466364 & 0.470758 & 0.252624 & 0.189510\\[0pt]
\end{tabular}
\end{table}

Computing and combining more than 3 SIS features is not rewarding of the computational expense.
Residuals are increasingly uncorrelated with the generated SIS features and model accuracy gains do not outstrip complexity.
However, in the process of creating Equation \ref{eq:bgexp}, 150 SIS predictor variables were determined and recorded.
50 primary predictors, 50 first residual predictors, and 50 second residual predictors.
These can serve as a high quality, introspective domain for the other architectures to fit on.

\subsection{Best Models on Engineered Domain}
\label{sec:org8867d46}
We set the aim of decreasing \(\mathcal{O}(n^3)\) computational expense of GPR by \(\approx\)10 times.
So, we aim to take 30 highly correlated features (slightly more than one half the number used by prior models) from these SIS subspaces.
We expected this to solve the problems inherent to the raw \ref{sec:orge40c1c6}.

fitting models to SIS features may leverage the denser and more continuous domain to improve extrapolative predictions.
Potentially into the high-entropy domain, or simply Theory.
However using the SIS subspaces in this way compromises on SISSO's explicability and necessitates SHAP analysis.
Unfortunately, whatever the gains in training time complexity and extrapolative ability, the models underperformed in predicting band gap in the cardinal mixing domain (see Table \ref{tbl:LoTscores}).
This was unexpected considering the raw features are by their nature highly correlated and presumed redundant.
Nevertheless, the RFR model on the higher dimensional, sparser raw features is superior.

 
\begin{figure}[htbp]
\centering
\includesvg[inkscapeformat=png, inkscapedpi=300,width=450pt]{./.ob-jupyter/3e69dc16f31bdc9d9941843a5f8a7a3ee3884e63}
\caption{\label{fig:sis-pairplots} SIS-based model predictions vs true values at multiple fidelities}
\end{figure}

\section{Discussion}
\label{sec:org32df119}
\subsection{SHAP Analysis of Domain}
\label{sec:org5a25e22}
SHAP scores are computed automatically for every dimension of every sample in the domain by the python SHAP package\footnote{\url{https://github.com/slundberg/shap}}.
The sum of the expectation value of the target conditioned on the model features and the  SHAP scores computed for each predictor variable of a sample is the model's  prediction for that sample target.
\autocite{lundberg-2017-unified-approac}
For the perovskite band gap the expectation value is 2.836 when conditioned on the raw features and 2.863 when conditioned on the SIS features.
The raw features' SHAP values are more centered around zero while engineered features are more often scored decisively positive or negative.

Figures \ref{fig:rfrSHAP} and \ref{fig:gprSHAP} show the top score distributions.
In each figure, features are ranked by overall value on the y-axis.
The x-axis shows the SHAP score for each point.
The points are shaped in a violin plot to show the distribution of effects the presence of the given feature can have.
Finally, on the color-axis, feature value specifies whether a particular score is a large or small absolute contributor of the sum to the prediction.

For instance, in figure \ref{fig:rfrSHAP}, the B-site Electronegativity is often a strongly positive contributor to the RFR prediction.
However, almost always in this case it is out-contributed by other features, it does not mostly determine the result but it is still valuable.
On the contrary, when it is a strongly negative contributor it effectively determines the result.
It is interesting to see how models make use of features in light of basic bi-variate correlations.
The only features that correlate strongly with band gap are illustrated in figure \ref{fig:rpear}.
Notably, the Random Forest Regression (RFR) primarily uses the highly correlated features, while the Gaussian Process Regression (GPR) primarily uses features with lower Pearson correlations.

\begin{figure}[htbp]
\centering
\includegraphics[width=450pt]{/home/panos/Documents/manuscripts/DFT+ML+feature_engineering/RFR/.ob-jupyter/27cc6a3fc4eab6935fcc0988ea4ac382c4eb8147.png}
\caption{\label{fig:rfrSHAP} Random Forest Regression Band Gap SHAP Values}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=450pt]{/home/panos/Documents/manuscripts/DFT+ML+feature_engineering/GPR/.ob-jupyter/31552cf23170d5409f0d7f373221bd1784e2b209.png}
\caption{\label{fig:gprSHAP} Gaussian Process Regression Band Gap SHAP Values}
\end{figure}

 
\begin{figure}[htbp]
\centering
\includesvg[inkscapeformat=png, inkscapedpi=300,width=400pt]{./.ob-jupyter/f93593f90e4b5b8cccd7c1395b6e9977e884e8d9}
\caption{\label{fig:rpear} raw features with (\(|p| > 0.5\)) against band gap}
\end{figure}

SHAP scores in principle quantify the contributions of site members and site member properties to the perovskite band gap.
On a sample-by-sample basis it is possible to say how much of the bandgap is contributed by the presense of a given quantity of, for example, Germanium.
However a clustering analysis reveals no universal patterns.
SHAP scores given the raw domain are near zero on average regardless of partitions made by level of theory, alloy scheme, or presence of organic A-site occupants.
This analysis confirms the difficulty of deducing a rule of thumb for the synthesis of perovskites with desirable properties.
If anything, figure \ref{fig:clusters} confirms that the Iodine at the X site tends to slightly increase band gaps.

 
\begin{figure}[htbp]
\centering
\includesvg[width=450pt]{./.ob-jupyter/df6e096d29860b55fe5610a23627968785eebc23}
\caption{\label{fig:clusters} SHAP score distributions reveal effects of individual constituents}
\end{figure}

\subsection{Predictions and Screening}
\label{sec:orga6fb54c}
Using the superior RFR model, we made predictions of the band gap for all 37785 possible compositions demonstrating cardinal mixing within the bounds of a 2x2x2 perovskite super cell.
That is eight A-sites shared by up to 5 constituents, 8 B-sites shared by up to six constituents, and 24 X-sites shared by up to 3 constituents.
Given the good coverage achieved by our sample dataset (figure \ref{fig:coverage}) and according to the scores reported in Table \ref{tbl:LoTscores}, the RFR model is capable of predicting band gaps at the experimental fidelity with a 0.154448 rmse.
\autocite{yang-2023-high-throug}
These predictions were projected on the sample space in Figure \ref{fig:pred}.

 
\begin{figure}[htbp]
\centering
\includesvg[width=450pt]{./.ob-jupyter/c2fa740c28da21ebb85c0902aea8c9b9ccbf3952}
\caption{\label{fig:pred} Band gap predictions overlaid on cardinal mixing chemical domain projected from fourteen to two dimensions via t-SNE}
\end{figure}

We followed a similar high-throughput screening procedure to that laid out in prior works.
\autocite{yang-2023-high-throug,mannodi-kanakkithodi-2021-comput-data}
We selected for band gaps between 1.0 and 2.5 eV as this range is expected to yield the best power conversion efficiency (PCE) in the visible spectrum.
\autocite{yu-2012-ident-poten,shockley-1961-detail-balan}
Perovskite compounds were selected for their predicted stability by cutting on each of three tolerance factors.
Namely the Goldschmidt's tolerance, the octahedral tolerance, and the tolerance proposed by \textcite{bartel-2019-new-toler}.

These cuts trimmed the data set from \textasciitilde{}40000 points to a subset of 3247 viable candidates.
These selected candidates were projected onto the domain space in Figure \ref{fig:chosen}.
A Frequency analysis revealed the constituent elements of the chosen subset most often occupied either small or large shares of their site.
Most A-site constituents preferred occupying 1/8\textsuperscript{th} of their site at a rate of about 8\%, with Potassium and Rb also preferring full occupancy 10-12\% of the time.
B-site constituents favored pure configurations at a rate of 5-8\% but also showed some preference for doping configurations.
X-site constituents, however showed very strong preference for fully occupying their site 25\% of the time.
See figure \ref{fig:freq}.

 
\begin{figure}[htbp]
\centering
\includesvg[width=450pt]{./.ob-jupyter/1efb3d52d06615061a39bb4b1d85bf1336016744}
\caption{\label{fig:freq} Frequency of mixing fractions of species at the A, B, and X sites across the \textasciitilde{}3000 screen compounds}
\end{figure}

\chapter{CONCLUSIONS}
\label{sec:org73c67ba}
