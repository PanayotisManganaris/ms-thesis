\chapter{PRESENTATIONS}
\label{sec:orgcd78d4b}
\begin{itemize}
\item Poster for DS02 symposium at MRS Boston Fall 2022
\citetitle{2022-novel-halid}\autocite{2022-novel-halid}
\item Talk for Purdue Soft Materials symposium
\citetitle{2022-machin-learn}\autocite{2022-machin-learn}
\item Developed notebooks
\autocite{manganaris-2022-mrs-comput}
for MRS Honolulu Spring 2022 tutorial hosted on nanoHUB
\citetitle{2022-ds00-tutor}\autocite{2022-ds00-tutor}
\end{itemize}

\chapter{SOFTWARE AND DATA CONTRIBUTIONS}
\label{sec:orga3ae64f}
\section{Data Publication}
\chapter{PUBLICATIONS}
\label{sec:org052c030}
I published the prepared sample data I used to the Materials Data Facility.
It is available to download with a simple API call following installation and activation\footnote{\url{https://ai-materials-and-chemistry.gitbook.io/foundry/}} of the relevant packages and applications.

\begin{ZZlisting}
  \caption{\label{lst:MDF} How to load the Mannodi Group halide perovskites data set from the Materials Data Facility repository}
  \begin{CenteredBox}
    \begin{lstlisting}[language=python]
# THIS IS PENDING
f = Foundry(
    no_local_server=True,
    no_browser=True,
    globus=True,
    index="mdf"
)
f.load("foundry_mrg_band_gaps_v1.0", globus=globus)
res = f.load_data()
X_mp, y_mp = res['train'][0], res['train'][1]
    \end{lstlisting}
  \end{CenteredBox}
\end{ZZlisting}

\section{Software Tools}
\label{sec:org85fe2a5}
I have made available the following python libraries to ease aggregation, sharing, analysis and reporting of large computational datasets.

The \texttt{cmcl} library\footnote{\url{https://github.com/PanayotisManganaris/cmcl}} is under early development under tag v0.1.5.
At this stage, cmcl strives to provide an inquisitive interface to perovskite composition feature computers in the style of the pandas API.
Listing \ref{lst:cmcl} demonstrates its use in extracting composition vectors from the formula strings identifying each compound in a dataset.

A library of model evaluation tools to assist with exhaustive grid search is being maintained in the \texttt{yogi} repository.\footnote{\url{https://github.com/PanayotisManganaris/yogi}}
A grid-search assistant under the \texttt{yogi.model\_selection.butler} module was used to optimize the hyper-parameters of models reported here.
See documentation for the various grid-narrowing strategies available.
\nocite{manganaris-2022-mrs-comput}
\nocite{yang-2023-high-throug}
\nocite{manganaris-2023-multi-fidel}
\nocite{gollapalli-2023-graph-neural}
\nocite{edlabadkar-2023-drivin-halid}
\nocite{yang-2023-discov-novel}

Matgenix\footnote{\url{https://github.com/Matgenix/pysisso}} company initially created a SciKit-learn compliant interface to the SISSO algorithm maintained by \textcite{ouyang-2018-sisso}.
This code was forked and modified\footnote{\url{https://github.com/PanayotisManganaris/pysisso}} to enable the creation of the SIS domain engineered regressions primarily by creating a custom \texttt{Function Transformer} that may be seeded with the subspace information obtained by first training a SISSO regression using the conventional interface.
This was done with the expectation that the resulting model would be superior for the purposes of this work, but it is generally applicable to any other applications demanding this sort of dimensionality reduction.
\makeatletter
\defbibenvironment{bibliography}
  {\list
     {\printtext[labelnumberwidth]{%
      \printfield{labelprefix}%
      \printfield{labelnumber}}}
     {\setlength{\bibhang}{1in} %%%%% was 0pt
      \setlength{\itemindent}{1in}%  -\leftmargin %%%%% was 0pt
      \setlength{\itemsep}{\bibitemsep}%
      \setlength{\leftmargin}{0pt}%  .22in % 0.42in
      \setlength{\parsep}{\bibparsep}%
      \setlength{\rightmargin}{0.33in}%
      }%
      \renewcommand*{\makelabel}[1]{\hss##1}}
  {\endlist}
  {\item}
\makeatother

\section{Tutorials}
\label{sec:org5689b18}
I contributed to the development of a tutorial delivered in the Spring 2022 Materials Research Society conference.
\fullcite{manganaris-2022-mrs-comput}
This tutorial covered a variety of machine learning methods.
It provides a detailed example of the use of \texttt{cmcl} and \texttt{yogi} in feature extraction and training multi-fidelity random forest models by score weighting respectively.
The relevant notebooks were published on the Purdue university nanoHUB.
\printbibliography[heading=none,category=myarticles]
