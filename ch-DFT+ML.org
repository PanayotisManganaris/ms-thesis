#+options: ':nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline
#+options: author:t broken-links:nil c:nil creator:nil
#+options: d:(not "LOGBOOK") date:t e:t email:nil f:t inline:t num:t
#+options: p:nil pri:nil prop:nil stat:t tags:t tasks:("TODO" "DONE" "NEXT") tex:t
#+options: timestamp:t title:t toc:t todo:nil |:t
#+title: ch-DFT+ML
#+date: <2023-05-10 Wed>
#+author: Panayotis Manganaris
#+email: panos.manganaris@gmail.com
#+language: en
#+select_tags: export
#+exclude_tags: noexport
#+creator: Emacs 28.2 (Org mode 9.6.5)
#+cite_export: biblatex
#+latex_class: reportchapter
#+latex_class_options:
#+latex_header:
#+latex_header_extra:
#+description:
#+keywords:
#+subtitle:
#+latex_engraved_theme:
#+latex_compiler: pdflatex
#+date: \today
#+PROPERTY: header-args:jupyter-python :session mrg :kernel mrg :pandoc org :async yes
#+PROPERTY: header-args :results scalar drawer :eval never-export :exports results
# Intended for ONLY body export
* COMMENT dependencies
#+INCLUDE: /home/panos/Documents/manuscripts/DFT+ML+feature_engineering/dependencies.org
#+begin_src jupyter-python
  sys.path.append(os.path.expanduser("~/src/pysisso"))
#+end_src

#+RESULTS:
:results:
:end:

* COMMENT load sample and spaces
#+begin_src jupyter-python
  Xt = pd.read_csv("./X_t.csv", index_col=0)
  Xc = pd.read_csv("./X_c.csv", index_col=0)
  Xp = pd.read_csv("./X_p.csv", index_col=0)
  X = Xt
  Y = pd.read_csv("./Y.csv", index_col=0)
#+end_src

#+RESULTS:
:results:
:end:

#+begin_src jupyter-python
  X_big = pd.read_csv("./X_card.csv", index_col=0)
  Y_big = pd.read_csv("./Y_card.csv", index_col=0)
#+end_src

#+RESULTS:
:results:
:end:

#+begin_src jupyter-python
  XX = pd.read_csv("./X_processed.csv", index_col=0)
  XX_big = pd.read_csv("./X_card_processed.csv", index_col=0)
#+end_src

#+RESULTS:
:results:
:end:

* MODELS OF PEROVSKITE BAND GAP
:PROPERTIES:
:CUSTOM_ID: ML-bg
:END:

Novel halide perovskites with improved stability and optoelectronic properties can be designed via composition engineering at cation and/or anion sites.
Data-driven methods, especially involving high-throughput first principles computations and subsequent ML modeling using unique material descriptors, are key to achieving this goal.
I used a dataset consisting of -- among other characteristic properties -- simulated band gaps of a representative sample of halide perovskites (HaP).
The effects of mixing at different sites is described by the explicit fraction of a site occupied by a specific atomic or molecular species.
Also, a set of abstract features obtained as the weighted averages of these species' bulk physical properties is used to bolster the feature space.

The fidelity hierarchy in our data sample climbs from DFT simulations performed using the basic PBE GGA functional, to results obtained from physical experiments aggregated in literature.
[cite:@almora-2020-devic-perfor;@kim-2014-cdses-nanow;@swanson-2017-co-sublim]
Low fidelity data makes up the majority of the sample and serves as the foundation for interpolation.
However, it does not accuracy reproduce the experimental measurements.
My work leverages the data covered in chapters 1 and 2 to predict the band gap of arbitrary perovskite compositions at experimental actuary with little anticipated error.

To do this, a set of interpretable descriptors of each perovskite are used.
This takes the form of a 14-dimensional vector containing the atomic fractions of each of the 14 constituent species within the specified perovskite formula.
This vector is a sufficient descriptor of a perovskite and has served decent predictions.
[cite:@mannodi-kanakkithodi-2022-data-driven]
To improve regression I examine an addition 36 additional predictors derived from linear combinations of compositions and elemental properties obtained from the trusted Mendeleev databases.
[cite:@mentel-2014]

** CANCELED COMMENT Multi-Fidelity Learning 
:LOGBOOK:
- State "CANCELED"   from "TODO"       [2023-06-14 Wed 21:37] \\
  irrelevant
- State "DONE"       from "TODO"       [2022-12-21 Wed 11:26]
- State "DONE"       from "TODO"       [2022-09-13 Tue 12:51]
:END:
The state of the art in materials modeling favors Graph Neural Network (GNN) architectures.
[cite:@chen-2019-graph-networ;@choudhary-2021-atomis-line;@xie-2018-cryst-graph]
These deep learning models have sufficient flexibility to capture the continuous variability in relative positions of crystals and molecules.
They are effective models, but they are difficult to use with physical materials.
Accurately characterizing structures at a level of atomic granularity cannot be achieved even with state of the art 3D Electron Tomography techniques.
[cite:@ercius-2015-elect-tomog]
Yet, characterization of chemical composition is a well established practice, for example using X-ray spectroscopy.

Graph convolutional neural networks can power more accurate structure-target predictions at multiple fidelities [cite:@chen-2020-multi-fidel] by performing Multi-Task Learning (MTL).
For instance, this multiple-fidelity machine learning technique can infer the relationships between more plentiful PBE GGA data and rarer but more accurate HSE06 data based on a shared set of predicting features.
This relationship, if sufficiently general, can be used to reliably extrapolate from known points on the PBE co-domain to the unknown HSE co-domain.
Of course, while this is implemented successfully in neural networks, the concept holds for any model architecture that can simultaneously regress multidimensional targets which do not need to constitute one rectangular data structure.

Additionally, there are alternative approaches for learning from multiple fidelities of data that can be implemented on the domain side.
This circumvents the requirement for flexibility in encoding the co-domain.
For instance modeling the multiple outcomes as varying depending on a categorical variable representing the fidelity makes it possible to use a single target regression methods.
Our problem of accurately modeling low availability, high fidelity targets is approached in this way.

# Semi-supervised learning is a competing set of methods achieving
# similar outcomes.
# [cite:@chapelle-2006-semi-super-learn;@lee-2013-pseud-label]

We will employ the domain-side approach where the largest, lowest fidelity component of our dataset consists of density functional theory (DFT) band gap predictions made at the generalized gradient approximation (GGA) Perdew-Burke-Ernzerhof (PBE) level of theory.
On the other end, the smallest and highest fidelity subset of the sample consists of experimental measurements of physical devices collected from the literature.

** DONE Model Optimization
:LOGBOOK:
- State "DONE"       from "TODO"       [2022-12-21 Wed 16:56]
:END:
# not every scrap of data collected in the experiments was included in the sample
# standard cleaning and deduping operations need only be mentioned

The rigorous hyper-parameter Optimization (HPO) of any feature engineering and modeling pipeline is a problem discussed extensively in the literature.
HPO approaches can be broadly separated into exhaustive and efficient optimization strategies.
[cite:@yang-2020-hyper-optim]
We use a two-stage procedure for selecting the best model parameters.
The first stage is an exhaustive grid-search over diversely sampled parameter space.
Each combination of parameters instantiates a model which is then fit to each of a set of stratified training subsets generated by a K=3 K-fold split cross-validation strategy.
Every fitted model is subsequently cross-validated using a suite of regression scoring metrics applied to each LoT subset simultaneously using a custom SciKit-learn score adapter[fn:4].
The grid search is then narrowed to a high performance quadrant of the search space by the model evaluator based on recommendations made by a simple entropy minimization algorithm[fn:4].
The recommended grid quickly eliminates under-performing settings based on the sample probability of a setting appearing in a set of finalists according to the scoring rankings.
The selection score is additionally influenced by a weighted sum of the scoring ranks allowing for considerable tuning of the selection criterion.
For best results, a few different grid spaces were explored to corroborate eliminations.
After the recommendation is made, the granularity of the grid is increased in the remaining ambiguous parameters and the process is repeated.
In general, no more than 2 or 3 exhaustive searches are needed over a given set of grids.
Past this point, continuously variable hyper parameters can be individually optimized by plotting validation curves.

** COMMENT compute pearson
#+begin_src jupyter-python
  lot_compared = Y.groupby(
      "Formula", as_index=False
  ).apply(
      lambda df: df[["Formula", "bg_eV", "LoT"]].set_index(["Formula", "LoT"]).unstack("LoT")
  )
  lot_compared.index = lot_compared.index.droplevel(0)
  lot_compared.columns = lot_compared.columns.droplevel(0)
  lot_compared = lot_compared.reset_index()
#+end_src

#+RESULTS:
:results:
:end:

#+begin_src jupyter-python
  formX = pd.concat([Y.Formula, XX], axis=1)# .reindex(index=lot_compared.index)
  df = pd.merge(left=lot_compared, right=formX, how='left', on='Formula').set_index(
      [c for c in formX.columns if not c in formX.select_dtypes(np.number).columns]
  )
#+end_src

#+RESULTS:
:results:
:end:
  
#+begin_src jupyter-python
  pearson = pd.DataFrame(np.corrcoef(df.fillna(0), rowvar=False),
                         columns=df.columns,
                         index=df.columns)
#+end_src

#+RESULTS:
:results:
:end:

** DONE Featurization of Chemistries
:LOGBOOK:
- State "DONE"       from "TODO"       [2022-12-21 Wed 15:39]
:END:
For \alpha total A-site constituents represented in the whole database, \beta total B-site constituents, and \gamma total X-site constituents, we provide a Python tool[fn:3] which robustly coverts the composition string of each data point into a \(\alpha + \beta + \gamma\) dimensional composition vector.
In the case of our total dataset description \(\alpha + \beta + \gamma = 14\).
[cite:@yang-2022-high-throug]
In a subset of the data, the chemical vector (listing ref:lst:cdf) is produced using cmcl (listing ref:lst:cmcl).

#+begin_export latex
\begin{ZZlisting}
  \caption{\label{lst:cmcl} An example of the cmcl "ft" feature accessor}
  \begin{CenteredBox}
    \begin{lstlisting}[language=python]
import cmcl
Y = load_codomain_subset()
df = Y.Formula.to_frame().ft.comp()
df.index = Y.Formula
print(df)
    \end{lstlisting}
  \end{CenteredBox}
\end{ZZlisting}
#+end_export

#+begin_export latex
\begin{ZZlisting}
  \caption{\label{lst:cdf} Data frame of composition vectors generated by cmcl}
  \begin{CenteredBox}
    \begin{lstlisting}
                    FA   Pb   Sn    I   MA   Br
Formula                                        
FAPb_0.7Sn_0.3I_3  1.0  0.7  0.3  3.0  NaN  NaN
MAPb(I0.9Br0.1)3   NaN  1.0  NaN  2.7  1.0  0.3
    \end{lstlisting}
  \end{CenteredBox}
\end{ZZlisting}
#+end_export

This is naturally a sparse, relatively high dimensional descriptor.
With any growth in the composition space it becomes sparser.
This descriptor has been shown to be effective for interpolating the properties of irregularly mixed large supercells.
[cite:@mannodi-kanakkithodi-2022-data-driven]
However, a spare descriptor is generally bad for extrapolative modeling.
[cite:@ghiringhelli-2015-big-data] 

When extrapolation is the aim, continuously distributed, unique, and linearly independent features are much more reliable.
[cite:@lux-2020-inter-spars] 

Our attempts to provide a domain with these characteristics results in a raw feature space containing the following.
Fourteen sparse composition vectors extracted from chemical formula using =cmcl=[fn:3].
See figure ref:fig:pcomp.
Thirty-six dense site-averaged-property vectors computed as a linear combination of composition vectors and measured elemental properties.
[cite:@mentel-2014]
See figure ref:fig:pprop.
Finally, 5 categorical dimensions one-hot-encoding level of theory.
This provides the categorical axis for multi-task learning.
See table ref:tbl:LoTs.

#+begin_src jupyter-python :post wrap(*this*, w="250pt", c="label:fig:pcomp How composition vectors correlate with target bandgaps")
  sub = pearson.iloc[0:5, 5:19]
  p = px.imshow(sub, color_continuous_scale='RdBu_r', zmin=-1, zmax=1,
                labels=dict(color="Pearson Coefficient"),
                # text_auto='.2f'
                )
  p.update_xaxes(tickangle=-40, tickfont_size = 30,
                 tickmode='array',
                 tickvals=list(range(sub.columns.shape[0])),
                 ticktext=sub.columns.str.slice(6).str.replace("'", "").str.replace("_"," ")
                 )
  p.update_yaxes(tickangle=-40, tickfont_size = 30,
                 tickmode = 'array',
                 tickvals=list(range(sub.index.shape[0])),
                 ticktext=[l + " bg" for l in sub.index.to_list()]
                 )
  p.update_layout(
      coloraxis=dict(colorbar=dict(len=1, orientation='h', y=0.8)),
      margin=dict(t=0,b=0,l=0,r=0),
      font_size=20,
      # paper_bgcolor='rgba(255,255,255,0)',
      # plot_bgcolor='rgba(255,255,255,0)',
  )
  p.show(renderer='svg')
#+end_src

#+RESULTS:
:results:
 
#+attr_latex: :width 250pt
#+CAPTION: label:fig:pcomp How composition vectors correlate with target bandgaps
[[file:./.ob-jupyter/6035c4c998e735ff02c1615bf97afe8021a9fc1e.svg]]
:end:

#+begin_src jupyter-python :post wrap(*this*, w="400pt", c="label:fig:pprop How site-averaged-properties vectors correlate with target bandgaps")
  sub = pearson.iloc[0:5, 19:55]
  p = px.imshow(sub, color_continuous_scale='RdBu_r', zmin=-1, zmax=1,
                labels=dict(color="Pearson Coefficient"),
                height=400, width=1000, aspect='auto',
                # text_auto='.2f'
                )
  p.update_xaxes(tickangle=-40, # tickfont_size = 30,
                 tickmode='array',
                 tickvals=list(range(sub.columns.shape[0])),
                 ticktext=sub.columns.str.slice(6).str.replace("'", "").str.replace("_"," ")
                 )
  p.update_yaxes(tickangle=-40, # tickfont_size = 30,
                 tickmode = 'array',
                 tickvals=list(range(sub.index.shape[0])),
                 ticktext=[l + " bg" for l in sub.index.to_list()]
                 )
  p.update_layout(coloraxis=dict(colorbar=dict(len=1, orientation='h', y=1)))
  p.update_layout(
      margin=dict(t=0,b=0,l=0,r=0),
      font_size=20
      # paper_bgcolor='rgba(255,255,255,0)',
      # plot_bgcolor='rgba(255,255,255,0)',
  )
  p.show(renderer='svg')
#+end_src

#+RESULTS:
:results:
 
#+attr_latex: :width 400pt
#+CAPTION: label:fig:pprop How site-averaged-properties vectors correlate with target bandgaps
[[file:./.ob-jupyter/b1e989b1f8e5352cd90464130524ee512e9070d1.svg]]
:end:

** DONE Machine Learning Algorithms and Scoring Methodology
:LOGBOOK:
- State "DONE"       from "NEXT"       [2023-06-14 Wed 23:55]
- State "NEXT"       from "DONE"       [2023-05-30 Tue 12:32]
- State "DONE"       from "TODO"       [2022-12-21 Wed 16:26]
:END:
I trained Random Forest Regression (RFR) and Gaussian Process Regression (GPR) models to predict the band gap from the union of predictor features previously discussed.
I chose to use the implementations of each of these algorithms packaged with the SciKit-Learn v1.2 package for python.
[cite:@pedregosa-2011-scikit-learn] 
The hypothesis sets offered to the task by each of these architectures differ dramatically.
A RFR is a flexible nonlinear ensemble model consisting of decision trees, a simple algorithm that captures interactions between descriptors in the form of a comparative algorithm.
Each tree is trained on a random subsample of the training data, resulting in different algorithms.
The RFR prediction is made by following the algorithm of each tree to the target and averaging the results.
The advantage of this approach is that each tree is highly biased to the data allowing even minute outliers to be accounted for.
However, by using many of them, the variance in data can also be explained, thus reducing the variance of error.
Naturally, the RFR benefits from using as much data as possible with as many estimators as possible.

A GPR model is a principled linear model functioning very differently.
Informally, GPRs "remember" the training examples and judge unlabeled data by its similarity to that aggregate memory.
This is implemented as a kernel method leveraging some similarity function \(k(x, x')\).
It works out that This function defines a "universe" of functions with varying characteristics and, simultaneously a density of functions which can be interpreted as a Bayesian prior on function space.
Naturally, kernels require engineering to accommodate prior expectations. Additionally, they offer limited options for capturing non-differentiable stepwise functions.
Of course, the random forest offers nothing but stepwise functions thus posing as the opposite end of the two extremes.
By applying Bayes' rule with a likelihood function defined using the sampled data and solving for the posterior function distribution, a principled set of predictive functions is obtained.
Averaging these functions yields the mean prediction (identical to a Kriging model's ridge) and their variability gives principled estimates of the error at each point in the domain.
Due to this, the algorithm saturates after sufficient training and additional data ceases to benefit.
The primary advantage of this method is simply that it works for any two quantifiably similar \(x\), potentially vectors, text, or graphs.
This offers some possibilities for future research with enhanced features.
However this comes at the cost of \(\mathcal{O}(N^3)\) training time complexity and a break down in efficacy in sparse, high-dimensional spaces.

# Each model architecture is rigorously optimized with regard to both 1) generality over the domains of Perovskite compositions and site-averaged constituent properties and 2) generality over the domain of alloy classifications.
In order to monitor the performance of the regression during training nine metrics were used simultaneously to evaluate performance with respect to each fidelity and in the overall with attention to overall accuracy and maximum inaccuracy.
These scores were used throughout the hyper-parameter optimization to judge which parameters resulted in the best validation performance.
To train models to be more faithful to the highest fidelity, the score for that subset was weighted as more important.
So, eventually, only models that performed uniformly well on all alloy types and better on predicting the experimental dataset were selected.
From the various approaches tried, the best optimized model was selected to make experimental-quality predictions on all 37785 points in the sample space.
This procedure in demonstrated in an online notebook by [cite/text:@manganaris-2022-mrs-comput] hosted on the Purdue nanoHUB.

** DONE Feature Engineering
:LOGBOOK:
- State "DONE"       from "TODO"       [2022-12-21 Wed 16:51]
:END:
There has been success in creating analytical expressions for perovskite properties, particularly lattice parameters.
[cite:@jiang-2006-predic-lattic]
In an attempt to find an analytical predictor for band gap we employ the Sure Independence Screening and Sparsifying Operator (SISSO).
[cite:@ouyang-2018-sisso] 
SISSO is a generalization of "greedy pursuit" algorithms previously used for this purpose, namely orthogonal matching pursuit (OMP) and the Least Absolute Shrinkage and Selection Operator (LASSO) otherwise known as basis pursuit.
[cite:@tibshirani-1996-regres-shrin]
The SIS[fn:1] operator is a powerful application of compressed sensing and used to find a conceptually orthogonal basis of compound features that best explain the signal in some function.
[cite:@ghiringhelli-2017-learn-physic]
The SO is a potent dimensionality reduction, it does not perform any mathematical decomposition but instead picks existent dimensions that begin to approximate an orthogonal basis set.
It outperforms CUR decomposition by functioning effectively in extremely high rank vector spaces.
[cite:@ray-2021-various-dimen;@hamm-2019-cur-decom]
This is accomplished by posing the decomposition as a compressed sensing problem in the correlation metric space.
Together, these operators allows the program to effectively find candidates for a linearly independent basis in a vector space of immense size.
Unlike legacy techniques it does not suffer when features are correlated.
[cite:@tibshirani-1996-regres-shrin;@gauraha-2018-introd-to-lasso]
This high performance handling of highly correlated vectors makes it particularly appealing for use with the perovskite features.
The features illustrated in figures ref:fig:pprop are derived from those in figure ref:fig:pcomp.
Those composition vectors, due to the fact that they represent a unit formula are themselves correlated.
At the least, they trace a bounded space.

A full SISSO model produces a parsimonious model of the target property which is easy to interpret.
Subsequent applications of the SISSO operator to the residuals of the previous model serve as a clever interrogation of error[cite:@mayo-1998-error-growt] yielding additional terms that, at the cost of simplicity, better explain the target.
Notice, however, the large space of features contains more good explanatory features than are used in the final expression.
I extensively modified a SciKit-learn compliant [cite:@buitinck-2013-api] interface[fn:5] to the SISSO program originally developed by Matgenix[fn:6] for the purpose of better leveraging these cut explanatory features.
The goal of this approach was to overcome the limitations of the raw feature space by finding a basis of varied, unique, and descriptive features which could serve as the domains for more powerful estimators.
I planned to use this strategy to train SIS-augmented versions of the RFR and GPR models previously discussed.
I additionally hoped that this could help to cut down the total number of descriptors necessary, especially the sparse features.
To improve the interpretability of these, the SIS algorithm was restricted to combining raw features in ways that preserved their units, so to preserve overall interpretability.
SIS features complexity was restricted to a maximum of 3 operations primarily to encourage parsimonious descriptions.
The operations in table ref:tbl:ops were used to create compound features.

#+CAPTION: label:tbl:ops Operations for formation of combinatorial super-space
| Binary         | Unary             |
|----------------+-------------------|
| addition       | reciprocation     |
| subtraction    | power 2           |
| multiplication | power 3           |
| division       | natural logarithm |
|                | exponentiation    |
|                | root 2            |

** DONE Training and Evaluation Methodology
:LOGBOOK:
- State "DONE"       from "TODO"       [2023-06-05 Mon 21:55]
:END:
First, the sample set is shuffled to mitigate the models tendency to fit on sampling order.
Model training proceeded only after partitioning the dataset using an 80/20 train-test split.
The split was made in a stratified manner, ensuring both partitions contained a proportionate fraction of samples from each fidelity subset.
The test set of 282 points was held out for final evaluation.
The training used the remaining 1123 data points.
In order to specify the learning algorithm best at predicting the experimental fidelity from simulated data, a thorough grid-search of each algorithm's hyper-parameters was performed.
First an estimator pipeline was constructed as in figure ref:fig:pipe.
Any sparse vectors were made dense and NaNs were filled with zeros by a =SimpleImputer=.
The feature vectors were subject to l1 normalization so that the compound's stoichiometry converted to ratios.
The estimator at the end of the pipeline was instantiated with default parameters and later had optimal settings injected.

To find these settings, I opted for a \(K\)-folds validation strategy first necessitating an optimal value for \(K\) be found.
This was done empirically, first by generating learning curves with \(K=10\) for each estimator.
The knee on these plots indicates the minimum number of samples needed to train effectively on average.
The size of one fold would be the size of the validation set, with the remaining folds used in training.
So, I doubled the number of samples at the knee, subtracted it from the size of the total training partition and used the result to determine the size of one fold.
This method resulted in setting \(K = 3\) for GPR training and \(K = 4\) for RFR training.

#+DOWNLOADED: screenshot @ 2023-06-05 14:20:21
#+attr_caption: :width 300pt
#+CAPTION: label:fig:pipe SciKit-Learn pipeline terminating in a default random forest estimator
[[file:MACHINE_LEARNING_MODELS_OF_PEROVSKITE_BAND_GAP/2023-06-05_14-20-21_screenshot.png]]

Finally I performed hyper-parameter optimization by exhaustive grid search using methods discussed in section [[Model Optimization]].
Optimizing a model this way was expected to result in a good ability to interpolate perovskites properties in the well covered sample space detailed in figure ref:fig:coverage.
Ideally, the resulting model need only extrapolate over the one-hot-encoded LoT dimensions.
To confirm this, validation requires two approaches.

In order, the first approach was critical to confirm the model was capable of predicting properties of entirely unseen compositions.
A modified method of leave-one-out cross-validation was used to understand the distribution of possible errors.
The optimally parametrized model was retrained on a dataset derived from the train set where one particular combination of elements is not present at all in any stoichiometry and tasked to make predictions on a validation set of the excluded compounds.
To be clear, individual elements may be represented in the training set as long as they don't appear together in a single compound.
This strategy better tested the model's ability to comprehend novel compounds which it saw many of in the total sample space.
Additionally, this avoided the expense of true leave-one-out validation.
The second validation was to test that the model can accurately predict the values for specific unseen elements at different levels of theory.
In this case, test compounds need not be entirely unique because it was expected that the extrapolated predictions at higher levels of theory will leverage the better coverage of lower fidelity sample sets.
So, finally, the test set is used and the resulting predictions are evaluated for accuracy.
Shapley Additive Explaination (SHAP) analysis of the models lends insight to the average physical impacts of 1) site-specific alloying, 2) using organic molecules in the Perovskite superstructure, and 3) the distribution of effects that a level of theory has on the prediction.

** DONE Results
:PROPERTIES:
:CLASS: unnumbered
:END:
:LOGBOOK:
- State "DONE"       from "TODO"       [2023-05-19 Fri 09:26]
:END:
*** DONE COMMENT create subplots
:LOGBOOK:
- State "DONE"       from              [2022-12-21 Wed 13:11]
:END:
**** rfr
#+begin_src jupyter-python
  data = pd.read_csv(os.path.expanduser('~/data/perovskites/rfr_pred.csv'), index_col=0)
#+end_src

#+RESULTS:
:results:
:end:

#+begin_src jupyter-python :post wrap(*this*)
  x='true'
  y='pred'
  p1 = px.scatter(
      data[data.partition=='test'],
      x=x, y=y,
      hover_name="Formula",
      color="LoT"
  )
  xlims = min(data[x]), max(data[y])
  ylims = min(data[y]), max(data[y])
  p1.add_scatter(x = [min(xlims+ylims), max(xlims+ylims)],
                y = [min(xlims+ylims), max(xlims+ylims)],
                mode='lines', name="parity", marker=dict(color="black"),
                row='all', col='all')
  p1.show(renderer='svg')
#+end_src

#+RESULTS:
:results:
 
#+attr_latex: :width 300pt
#+CAPTION: 
[[file:./.ob-jupyter/a906bd083ffb28108985b0f92462f9e647fb61ac.svg]]
:end:

**** gpr
#+begin_src jupyter-python
  data = pd.read_csv(os.path.expanduser('~/data/perovskites/gpr_pred.csv'), index_col=0)
#+end_src

#+RESULTS:
:results:
:end:

#+begin_src jupyter-python :post wrap(*this*)
  x='true'
  y='pred'
  p2 = px.scatter(
      data[data.partition=='test'],
      x=x, y=y,
      hover_name="Formula",
      color="LoT"
  )
  xlims = min(data[x]), max(data[y])
  ylims = min(data[y]), max(data[y])
  p2.add_scatter(x = [min(xlims+ylims), max(xlims+ylims)],
                y = [min(xlims+ylims), max(xlims+ylims)],
                mode='lines', name="parity", marker=dict(color="black"),
                row='all', col='all')
  p2.show(renderer='svg')
#+end_src

#+RESULTS:
:results:
 
#+attr_latex: :width 300pt
#+CAPTION: 
[[file:./.ob-jupyter/3c9c5181f014f21b19616f66f427d89e3e8b7c56.svg]]
:end:

**** sisso
#+begin_src jupyter-python
  data = pd.read_csv(os.path.expanduser('~/data/perovskites/sisso_pred.csv'), index_col=0)
#+end_src

#+RESULTS:
:results:
:end:

#+begin_src jupyter-python :post wrap(*this*)
  x='true'
  y='pred'
  p3 = px.scatter(
      data[data.partition=='test'],
      x=x, y=y,
      hover_name="Formula",
      color="LoT"
  )
  xlims = min(data[x]), max(data[y])
  ylims = min(data[y]), max(data[y])
  p3.add_scatter(x = [min(xlims+ylims), max(xlims+ylims)],
                y = [min(xlims+ylims), max(xlims+ylims)],
                mode='lines', name="parity", marker=dict(color="black"),
                row='all', col='all')
  p3.show(renderer='svg')
#+end_src

#+RESULTS:
:results:
 
#+attr_latex: :width 300pt
#+CAPTION: 
[[file:./.ob-jupyter/5a513955767af532b6fba8ca0c676aebdc9e63bb.svg]]
:end:

*** DONE Best Models on Raw Domain
:LOGBOOK:
- State "DONE"       from              [2022-12-21 Wed 13:21]
:END:
# make sure to create subplots first
#+begin_src jupyter-python :post wrap(*this*, w="450pt", c="label:fig:pairplots model predictions vs true values at multiple fidelities")
  p0 = make_subplots(rows=1, cols=3, shared_yaxes=True)

  for trace1, trace2, trace3 in zip(p1.data, p2.data, p3.data):
      p0.add_trace(trace1, row=1, col=1) 
      p0.add_trace(trace2, row=1, col=2)
      p0.add_trace(trace3, row=1, col=3)

  p0.update_xaxes(tickvals=list(range(8)))
  p0.update_yaxes(row=1, col=1,
                  title="RFR Prediction",
                  title_font_size=24,
                  tickfont_size=20,
                  )
  p0.update_yaxes(row=1, col=2,
                  title="GPR Prediction", title_standoff=0,
                  title_font_size=24,
                  tickfont_size=20,
                  )
  p0.update_yaxes(row=1, col=3,
                  title="SISSO Prediction", title_standoff=0,
                  title_font_size=24,
                  tickfont_size=20,
                  )

  p0.layout.xaxis.scaleanchor="y"
  p0.layout.xaxis2.scaleanchor="y2"
  p0.layout.xaxis3.scaleanchor="y3"

  p0.update_traces(showlegend=False, row=1, col=1)
  p0.update_traces(showlegend=False, row=1, col=2)

  p0.update_xaxes(mirror=True,
                  constrain='domain', title='True Measurement',
                  title_font_size=24,
                  tickfont_size=20,)
  p0.update_yaxes(mirror=True)

  p0.update_traces(
      line=dict(color = 'black', width = 4),
  )
  p0.update_traces(
      marker_line=dict(color = 'black', width = 1),
      marker_size=6,
      selector={'marker_symbol':'circle'}
  )

  p0.update_layout(
      legend=dict(orientation='h', y=-0.4, itemsizing='constant'),
      margin=dict(t=70, b=0, l=0, r=0),
      width=700, height=360,
      title = "Band Gap [eV] (282 test predictions)"
  )

  p0.add_annotation(x=2.5, y=6, xref='x', yref='y',
                    text="r2 = 0.99<br>maxerr = 0.80<br>rmse = 0.12<br>", font_size=15,
                    showarrow=False)

  p0.add_annotation(x=2.5, y=6, xref='x2', yref='y',
                    text="r2 = 0.98<br>maxerr = 1.28<br>rmse = 0.15<br>", font_size=15,
                    showarrow=False)

  p0.add_annotation(x=2.5, y=6, xref='x3', yref='y',
                    text="r2 = 0.88<br>maxerr = 2.17<br>rmse = 0.47<br>", font_size=15,
                    showarrow=False)

  p0.show(renderer='svg')
#+end_src

#+RESULTS:
:results:
 
#+attr_latex: :width 450pt
#+CAPTION: label:fig:pairplots model predictions vs true values at multiple fidelities
[[file:./.ob-jupyter/1a3fd80c6a22dbce291d740dbb832e523b9d11eb.svg]]
:end:

Following HPO these models are finally validated against the test sets originally split off from the sample for both their extrapolative ability and consistency.
The random forest model was the best performing compared to the Gaussian process and SISSO regressions.
The analysis of its ability to make extrapolative predictions on completely unknown compositions is discussed in the following section [[Discussion]].
The RFR boasts an RMSE error on the total dataset of only 0.12 \unit{\electronvolt}.
This combined with its RMSE error of 0.15 \unit{\electronvolt} on the experimental fidelity subset promises this model can make quality predictions of the band gap at the experimental fidelity.
See table ref:tbl:LoTscores.
This error compares favorably with the best models currently ranked by the Materials Project's MatBench standard.
[cite:@dunn-2020-bench-mater]
Notably, these models use only the stoichiometry of the compositions while most MatBench models require atomic structures in addition to stoichiometric information.
Of course, these models are highly specialized to this sample space.
The RFR hyper-parameters are listed in the appendix (Table ref:tbl:rfrHPO).

The GPR model was tried with various kernels, both stationary and non-stationary, where each essentially describe the differentiability of the functions used in the posterior.
Ultimately, the best was a non-stationary Matern kernel with \(\nu = \frac{3}{2}\).
This kernel defines "rough" functions that are only once-differentiable.
This makes sense considering the inability of GPR to capture step functions and the stepwise nature of the LoT encoding dimensions.
Additionally, both the LoT dimensions and the composition vectors are sparse, which challenges the algorithm
Nevertheless, it is a close second to the RFR and offers a propitious start to models utilizing more advanced features.

*** DONE SISSO Model and SIS Engineered Features
:LOGBOOK:
- State "DONE"       from "TODO"       [2023-01-04 Wed 07:35]
:END:
The Sure Independence Screening and Sparsifying Operator (SISSO) is a specific combination of multiple data mining techniques chained together resulting in a symbolically expressed regression model.
[cite:@ouyang-2018-sisso;@ghiringhelli-2017-learn-physic] 

The best SISSO model for band gap involving 3 SIS features (each composed of up to 4 basic features) has an unremarkable RMSE of 0.476 eV, barely outperforming an OLS regression on 55 dimensions (see Table ref:tbl:LoTscores).
It is expressed in equation ref:eq:bgexp.
Notably, while the units of the expression do not match the units of band gap as measured (target units are unknown to the algorithm), they are still energy units.
This is by design, as the combination of features was restricted so to only allow compatible units to be combined.
A separate training session without this restriction was attempted, but the resulting model's performance was worse.

#+begin_src jupyter-python
  def mbox_ammend(string):
      matchlist = re.findall(r'\b(?!nonumber\b)[a-zA-Z\s]{3,}', string)
      m = re.search(matchlist[0], string)
      for substr in matchlist[1:]:
          string = string[:m.start(0)] + '\mbox{' + m[0] + '}' + string[m.end(0):]
          pat = re.compile(substr)
          m = pat.search(string, m.end(0))
      string = string[:m.start(0)] + '\mbox{' + m[0] + '}' + string[m.end(0):]
      return string

  def typeset_sisso_model(cpipe):
      return '+'.join(
          map(
              lambda x: x[1:-1], 
              map(str,zip(cpipe[-1].sisso_out.model.coefficients[0],
                          map(lambda x: "&" + str(x) + "\nonumber\\", cpipe[-1].sisso_out.model.descriptors)))
          )
      ).replace(",", "").replace("_", " ").replace("'", "") + "+" + "".join(
          map(str, cpipe[-1].sisso_out.model.intercept)
      )
#+end_src

#+RESULTS:
:results:
:end:

#+begin_src jupyter-python
  cpipe = joblib.load("./Models/sisso_bg.joblib")
  print(
      "bg\si{\electronvolt} = " + mbox_ammend(typeset_sisso_model(cpipe).replace(" kJpermol", ""))
  )
#+end_src

\begin{align}
\label{eq:bgexp}
bg\,\si{\electronvolt} = 1.752393064 &((X;\mbox{electronegativity}*A;\mbox{heat of fusion})-\nonumber\\&(B;\mbox{electron affinity}+B;\mbox{ionization energy}))\nonumber\\+-0.5862929089 &((B;Sn-\mbox{HSE})+(\mbox{PBE}-X;\mbox{electronegativity}))\nonumber\\+1.063684923 &((A;\mbox{electronegativity}-B;Ca)*(B;\mbox{heat of vap}-X;\mbox{electron affinity}))\nonumber\\+4.657097107
\end{align}

#+CAPTION: label:tbl:LoTscores RMSE of models on raw domain calculated per LoT subset
| rmse scores  |  GPR |  RFR | Linear OLS | SISSO | SIS + GPR | SIS + RFR |
|--------------+------+------+------------+-------+-----------+-----------|
| total        | 0.15 | 0.12 |       0.49 |  0.47 |      0.25 |      0.18 |
| EXP          | 0.12 | 0.15 |       0.30 |  0.33 |      0.33 |      0.23 |
| PBE          | 0.12 | 0.10 |       0.47 |  0.39 |      0.17 |      0.13 |
| HSE          | 0.21 | 0.15 |       0.55 |  0.51 |      0.30 |      0.20 |
| HSE(SOC)     | 0.15 | 0.10 |       0.53 |  0.57 |      0.27 |      0.22 |
| HSE-PBE(SOC) | 0.13 | 0.13 |       0.46 |  0.47 |      0.25 |      0.18 |

Computing and combining more than 3 SIS features is not rewarding of the computational expense.
Residuals are increasingly uncorrelated with the generated SIS features and model accuracy gains do not outstrip complexity.
However, in the process of creating Equation ref:eq:bgexp, 150 SIS predictor variables were determined and recorded.
50 primary predictors, 50 first residual predictors, and 50 second residual predictors.
These can serve as a high quality, introspective domain for the other architectures to fit on.

*** DONE COMMENT create subplots
:LOGBOOK:
- State "DONE"       from "NEXT"       [2023-04-25 Tue 18:33]
- State "NEXT"       from "TODO"       [2023-04-25 Tue 14:41]
:END:
**** rfr
#+begin_src jupyter-python
  data = pd.read_csv(os.path.expanduser('~/data/perovskites/rfr_sis_pred.csv'), index_col=0)
#+end_src

#+RESULTS:
:results:
:end:

#+begin_src jupyter-python :post wrap(*this*)
  x='true'
  y='pred'
  p1 = px.scatter(
      data[data.partition=='test'],
      x=x, y=y,
      hover_name="Formula",
      color="LoT"
  )
  xlims = min(data[x]), max(data[y])
  ylims = min(data[y]), max(data[y])
  p1.add_scatter(x = [min(xlims+ylims), max(xlims+ylims)],
                y = [min(xlims+ylims), max(xlims+ylims)],
                mode='lines', name="parity", marker=dict(color="black"),
                row='all', col='all')
  p1.show(renderer='svg')
#+end_src

#+RESULTS:
:results:
 
#+attr_latex: :width 300pt
#+CAPTION: 
[[file:./.ob-jupyter/2c6e16e0b3ffe6185e085c502cd6a12e91ce982a.svg]]
:end:

**** gpr
#+begin_src jupyter-python
  data = pd.read_csv(os.path.expanduser('~/data/perovskites/gpr_sis_pred.csv'), index_col=0)
#+end_src

#+RESULTS:
:results:
:end:

#+begin_src jupyter-python :post wrap(*this*)
  x='true'
  y='pred'
  p2 = px.scatter(
      data[data.partition=='test'],
      x=x, y=y,
      hover_name="Formula",
      color="LoT"
  )
  xlims = min(data[x]), max(data[y])
  ylims = min(data[y]), max(data[y])
  p2.add_scatter(x = [min(xlims+ylims), max(xlims+ylims)],
                y = [min(xlims+ylims), max(xlims+ylims)],
                mode='lines', name="parity", marker=dict(color="black"),
                row='all', col='all')
  p2.show(renderer='svg')
#+end_src

#+RESULTS:
:results:
 
#+attr_latex: :width 300pt
#+CAPTION: 
[[file:./.ob-jupyter/171d8c4dcb9a82035e0aefc58ff9aae084505aab.svg]]
:end:

*** DONE Best Models on Engineered Domain
:LOGBOOK:
- State "DONE"       from "NEXT"       [2023-04-25 Tue 18:33]
- State "NEXT"       from "TODO"       [2023-04-25 Tue 14:41]
:END:
We set the aim of decreasing \(\mathcal{O}(n^3)\) computational expense of GPR by \approx{}10 times.
So, we aim to take 30 highly correlated features (slightly more than one half the number used by prior models) from these SIS subspaces.
We expected this to solve the problems inherent to the raw features obtained in section [[Featurization of Chemistries]].

Fitting models to SIS features may leverage the denser and more continuous domain to improve extrapolative predictions.
Potentially into the high-entropy domain, or simply Theory.
However using the SIS subspaces in this way compromises on SISSO's explicability and necessitates SHAP analysis.
Unfortunately, whatever the gains in training time complexity and extrapolative ability, the models underperformed in predicting band gap in the cardinal mixing domain (see Table ref:tbl:LoTscores).
This was unexpected considering the raw features are by their nature highly correlated and presumed redundant.
Nevertheless, the RFR model on the higher dimensional, sparser raw features is superior.

#+begin_src jupyter-python :post wrap(*this*, w="450pt", c="label:fig:sis-pairplots SIS-based model predictions vs true values at multiple fidelities")
  p0 = make_subplots(rows=1, cols=2, shared_yaxes=True)

  for trace1, trace2, trace3 in zip(p1.data, p2.data, p3.data):
      p0.add_trace(trace1, row=1, col=1) 
      p0.add_trace(trace2, row=1, col=2)

  p0.update_xaxes(tickvals=list(range(8)))
  p0.update_yaxes(row=1, col=1,
                  title="RFR Prediction",
                  title_font_size=24,
                  tickfont_size=20,
                  )
  p0.update_yaxes(row=1, col=2,
                  title="GPR Prediction", title_standoff=0,
                  title_font_size=24,
                  tickfont_size=20,
                  )

  p0.layout.xaxis.scaleanchor="y"
  p0.layout.xaxis2.scaleanchor="y2"

  p0.update_traces(showlegend=False, row=1, col=1)

  p0.update_xaxes(mirror=True,
                  constrain='domain', title='True Measurement',
                  title_font_size=24,
                  tickfont_size=20,)
  p0.update_yaxes(mirror=True)

  p0.update_traces(
      line=dict(color = 'black', width = 4),
  )
  p0.update_traces(
      marker_line=dict(color = 'black', width = 1),
      marker_size=6,
      selector={'marker_symbol':'circle'}
  )

  p0.update_layout(
      legend=dict(# orientation='h', y=-0.35
                  itemsizing='constant'),
      margin=dict(t=70, b=0, l=0, r=0),
      width=660, height=330,
      title = "Band Gap [eV] (282 test predictions)"
  )

  p0.add_annotation(x=2.5, y=6, xref='x', yref='y',
                    text="r2 = 0.98<br>maxerr = 1.18<br>rmse = 0.18<br>", font_size=15,
                    showarrow=False)

  p0.add_annotation(x=2.5, y=6, xref='x2', yref='y',
                    text="r2 = 0.97<br>maxerr = 1.68<br>rmse = 0.25<br>", font_size=15,
                    showarrow=False)

  p0.show(renderer='svg')
#+end_src

#+RESULTS:
:results:
 
#+attr_latex: :width 450pt
#+CAPTION: label:fig:sis-pairplots SIS-based model predictions vs true values at multiple fidelities
[[file:./.ob-jupyter/383e3b86e9b2d36279558672c6ca56306fae9032.svg]]
:end:

** DONE Discussion
:PROPERTIES:
:CLASS: unnumbered
:END:
:LOGBOOK:
- State "DONE"       from "TODO"       [2023-05-19 Fri 11:13]
:END:
*** COMMENT perform validation
#+begin_src jupyter-python
  cpipe = joblib.load("./Models/rfr_t_bg.joblib")
#+end_src

#+RESULTS:
:results:
:end:

#+begin_src jupyter-python
  sss = StratifiedShuffleSplit(n_splits=1, train_size=0.8, random_state=1111)
  train_idx, test_idx = next(sss.split(X, Y.lotord)) #stratify split by LoT categories
  X_tr, X_ts = X.iloc[train_idx], X.iloc[test_idx]
  Y_tr, Y_ts = Y.iloc[train_idx], Y.iloc[test_idx]
#+end_src

#+RESULTS:
:results:
:end:

#+begin_src jupyter-python 
  erdf = run_cv_report(cpipe, GroupKFold(n_splits=366), lot_scorings,
                       X_tr, Y_tr[target].iloc[:,0], Y_tr.memord, Y_tr.members)  
#+end_src

#+RESULTS:
:results:
:end:

#+begin_src jupyter-python
  pdf = pd.concat([
      pd.merge(erdf.reset_index()['index'].str.split(' ').str[-1],
               Y_tr.members.value_counts().reset_index(),
               on=['index'], how='left').drop('index', axis=1),
      erdf.reset_index(),
  ], axis=1)
#+end_src

#+RESULTS:
:results:
:end:

#+begin_src jupyter-python
  pdf.loc[:, pdf.columns.str.contains("rmse", regex=True)] = pdf.loc[:, pdf.columns.str.contains("rmse", regex=True)] * -1
#+end_src

#+RESULTS:
:results:
:end:

#+begin_src jupyter-python
  pdf['index'] = [
      combo if count > 7 else 'rest of candidate<br>combinations' for combo, count in zip(
          pdf['index'].to_list(), pdf.members.to_list()
      )
  ]
#+end_src

#+RESULTS:
:results:
:end:

*** DONE Validation of Expected Error
:LOGBOOK:
- State "DONE"       from "TODO"       [2023-06-15 Thu 01:28]
:END:
The errors reported in the prior section are promising but questionable due to the lack of testing of the model's ability to make predictions on completely novel compounds.
Following the appropriate validation methodology outlined in section [[Training and Evaluation Methodology]] addresses this concern.
This results in a distribution of errors with a mean EXP RMSE of about 0.2 \unit{\electronvolt}, which is only slightly worse than error obtained in the testing.
The mean scores are better than the median, but this is mostly due to a small number of outlier compounds which are not very exotic, so it is unrealistic not to train on at least one example of them at a simulation fidelity.
See figure ref:fig:errval.
There is definitely a loss in validation performance as compared to the scores on the train set, see table ref:tbl:errval.
Notice, the \(R^2\) score is missing because it can not be computed on validation sets that contain only a single sample.
The explained variance (ev) score is poor on the test set, but nearly perfect on the train set.
This is the biggest difference in the scoring by far, simply due to the higher variability of errors in the test sets.
Certainly, the interpolation demanded of the model will not be perfect on wholly unseen compositions, but it seems that in the majority of instances, the prediction can be justifiably expected to be reasonably accurate.

#+begin_src jupyter-python :post wrap(*this*, w="400pt", c="label:fig:errval Distribution of leave-one-composition-out cross-validation errors weighted by the size of validation sets")
    p = px.histogram(
        pdf[pdf.partition=='test'],
        x='rmse_EXP',
        y='members',
        facet_col='partition',
        color='index',
        text_auto=True,
        width=800,
        height=450,
        # title="Distirbution of errors weighted by size of validation sets"
    )
    p.update_traces(texttemplate='%{y}', textposition='outside')
    p.update_layout(legend=dict(font_size=14.5, title='Compound Members'),
                    margin=dict(t=0))
    p.update_yaxes(title='sum of validation set sizes')
    p.update_xaxes(title='RMSE of experimental-fidelity prediction')
    p.for_each_annotation(lambda a: a.update(text=""))
    p.show(renderer='svg')
#+end_src

#+RESULTS:
:results:
 
#+attr_latex: :width 400pt
#+CAPTION: label:fig:errval Distribution of leave-one-composition-out cross-validation errors weighted by the size of validation sets
[[file:./.ob-jupyter/d44a36fb406250843e09ef8033a779d1b729d413.svg]]
:end:

#+begin_src jupyter-python :post wraptbl(c="label:tbl:errval Leave-one-composition-out cross-validation scored by complete suite")
  wm = lambda x: np.average(x, weights=pdf.loc[x.index, "members"])
  pdf.groupby('partition').agg(
      {col:wm for col in pdf.columns if not col in ['r2', 'members', 'index', 'partition']}
  ).T
#+end_src

#+RESULTS:
:results:
 
#+CAPTION: label:tbl:errval Leave-one-composition-out cross-validation scored by complete suite 
| partition            |  test | train |
|----------------------+-------+-------|
| ev                   | -2.48 |  0.99 |
| maxerr               |  0.32 |  0.96 |
| rmse                 |  0.22 |  0.10 |
| rmse EXP             |  0.22 |  0.06 |
| rmse PBE             |  0.18 |  0.11 |
| rmse HSE             |  0.22 |  0.11 |
| rmse HSE(SOC)        |  0.21 |  0.10 |
| rmse HSE-PBErel(SOC) |  0.20 |  0.10 |
:end:

*** DONE SHAP Analysis of Domain
:LOGBOOK:
- State "DONE"       from "NEXT"       [2023-05-19 Fri 08:50]
- State "NEXT"       from "TODO"       [2023-04-25 Tue 19:10]
- State "DONE"       from "TODO"       [2022-12-21 Wed 22:20]
:END:
SHAP scores are computed automatically for every dimension of every sample in the domain by the python SHAP package[fn:9].
The sum of the expectation value of the target conditioned on the model features and the  SHAP scores computed for each predictor variable of a sample is the model's  prediction for that sample target.
[cite:@lundberg-2017-unified-approac]
For the perovskite band gap the expectation value is 2.836 when conditioned on the raw features and 2.863 when conditioned on the SIS features.
The raw features' SHAP values are more centered around zero while engineered features are more often scored decisively positive or negative.

Figures ref:fig:rfrSHAP and ref:fig:gprSHAP show the top score distributions.
In each figure, features are ranked by overall value on the y-axis.
The x-axis shows the SHAP score for each point.
The points are shaped in a violin plot to show the distribution of effects the presence of the given feature can have.
Finally, on the color-axis, feature value specifies whether a particular score is a large or small absolute contributor of the sum to the prediction.

For instance, in figure ref:fig:rfrSHAP, the B-site Electronegativity is a strongly positive contributor to the RFR prediction.
Large B-site electronegativities tend to result in a subtraction from the mean band gap of about 0.5 \unit{\electronvolt}.
Small electronegativities tend to result in an addition to the mean band gap averaging 1.5 \unit{\electronvolt} but with much wider variability.
It is interesting to see how models make use of features in light of basic bi-variate correlations.
The only features that correlate strongly with band gap are summarized in figure ref:fig:rpear.
Notably, the Random Forest Regression (RFR) primarily uses the highly correlated features, while the Gaussian Process Regression (GPR) primarily uses features with lower Pearson correlations.

#+CAPTION: label:fig:rfrSHAP Random Forest Regression Band Gap SHAP Values
#+attr_latex: :width 450pt
[[file:~/Documents/manuscripts/DFT+ML+feature_engineering/RFR/.ob-jupyter/27cc6a3fc4eab6935fcc0988ea4ac382c4eb8147.png]]

#+CAPTION: label:fig:gprSHAP Gaussian Process Regression Band Gap SHAP Values
#+attr_latex: :width 450pt
[[file:~/Documents/manuscripts/DFT+ML+feature_engineering/GPR/.ob-jupyter/31552cf23170d5409f0d7f373221bd1784e2b209.png]]

#+begin_src jupyter-python
  XY = pd.concat([XX, Y], axis=1).select_dtypes(np.number).fillna(0)
  pearson = pd.DataFrame(np.corrcoef(XY, rowvar=False),
                         columns=XY.columns,
                         index=XY.columns)
  sub = pearson.iloc[56, 0:55]
  ytitles = ["Band Gap [eV]"]
  sub = sub.loc[(sub.abs()>0.5).apply(bool)].to_frame().T
#+end_src

#+RESULTS:
:results:
:end:

#+begin_src jupyter-python :post wrap(*this*, "400pt :options inkscapeformat=png, inkscapedpi=300", "label:fig:rpear raw features with (\\(|p| > 0.5\\)) against band gap")
  p = px.imshow(sub, color_continuous_scale='RdBu_r', zmin=-1, zmax=1,
                labels=dict(color="Pearson Coefficient"),
                y = ytitles, text_auto='.2f',
                height=300
                )
  p.update_xaxes(tickangle=-35)
  p.update_yaxes(tickangle=-35, )
  p.update_layout(coloraxis=dict(colorbar=dict(len=1, orientation='h', y=0.65)))
  p.update_layout(
      margin=dict(t=0,b=0,l=0,r=0),
  )
  p.update_layout(
      xaxis = dict(tickmode='array',
                   tickvals=list(range(sub.columns.shape[0])),
                   ticktext=sub.columns.str.slice(6).str.replace("'", "").str.replace("_"," "),
                   ),# font_size=40,
      # xaxis = dict(text_font_size=40)
  )
  p.show(renderer='svg')
#+end_src

#+RESULTS:
:results:
 
#+attr_latex: :width 400pt :options inkscapeformat=png, inkscapedpi=300
#+CAPTION: label:fig:rpear raw features with (\(|p| > 0.5\)) against band gap
[[file:./.ob-jupyter/f93593f90e4b5b8cccd7c1395b6e9977e884e8d9.svg]]
:end:

# Likewise, with respect to the generality measure conducted earlier, it seems the presence of individual elements is far more predictive of the total band gap than mix status.
# This would explain why X-site and A-site alloys are sufficient to predict the band gaps of B-site alloys, despite those groups containing no B-site alloys themselves, they do contain a representative sample of B-site elements.

SHAP scores in principle quantify the contributions of site members and site member properties to the perovskite band gap.
On a sample-by-sample basis it is possible to say how much of the bandgap is contributed by the presense of a given quantity of, for example, Germanium.
However a clustering analysis reveals no universal patterns.
SHAP scores given the raw domain are near zero on average regardless of partitions made by level of theory, alloy scheme, or presence of organic A-site occupants.
This analysis confirms the difficulty of deducing a rule of thumb for the synthesis of perovskites with desirable properties.
If anything, figure ref:fig:clusters confirms that the Iodine at the X site tends to slightly increase band gaps.

#+begin_src jupyter-python
  SV = pd.read_csv("./RFR_t_sv.csv", index_col=0)
#+end_src

#+RESULTS:
:results:
:end:

#+begin_src jupyter-python :post wrap(*this*, w="450pt", c="label:fig:clusters SHAP score distributions reveal effects of individual constituents")
  p = px.histogram(
      SV.iloc[:, 1:15].join(
          Y[['LoT','org','mix']]
      ).melt(
          id_vars=['LoT','org','mix']
      ),
      x='value',
      facet_col='variable',
      facet_col_wrap=4,
      facet_col_spacing=0.04,
      nbins=500,
      )
  p.update_xaxes(range=[-0.25, 0.25])
  p.update_yaxes(range=[0, 4])
  p.for_each_annotation(lambda a: a.update(
      text = a.text.split("__")[-1].replace("'", "")
  ))
  for i,j in zip([1,1,2,2], range(4)):
      p.update_xaxes(
          row=i, col=j+1, title="SHAP value",
          showticklabels=True,
      )

  for t in p.data:
      mean = np.mean(t.x)
      color = 'red'#t.marker.color
      xa, ya = t.xaxis, t.yaxis

      p.add_shape(
          go.layout.Shape(
              type='line', xref=xa, yref=ya,
              x0=mean, y0=0, x1=mean, y1=4,
              line=dict(
                  color=color,
                  width=2,
              )
          ),
      )
      p.add_annotation(
          x=mean+0.05, y=3, xref=xa, yref=ya,
          text=f"mean<br>{mean:.4f}",
          showarrow=False,
          xanchor='left'
      )

  p.show(renderer='svg')
#+end_src

#+RESULTS:
:results:
 
#+attr_latex: :width 450pt
#+CAPTION: label:fig:clusters SHAP score distributions reveal effects of individual constituents
[[file:./.ob-jupyter/df6e096d29860b55fe5610a23627968785eebc23.svg]]
:end:

*** COMMENT load screening
#+begin_src jupyter-python
  screen = pd.read_csv("./screen_rfr.csv", index_col=0)
#+end_src

#+RESULTS:
:results:
:end:

#+begin_src jupyter-python
  projdf = pd.read_csv("./proj_t_rfr_tsne_c.csv", index_col=0)
#+end_src

#+RESULTS:
:results:
:end:

*** DONE Predictions and Screening
:LOGBOOK:
- State "DONE"       from "TODO"       [2023-05-19 Fri 11:12]
:END:
Using the superior RFR model, I predict the band gap for all 37785 possible compositions demonstrating cardinal mixing within the bounds of a 2x2x2 perovskite super cell.
That is eight A-sites shared by up to 5 constituents, 8 B-sites shared by up to six constituents, and 24 X-sites shared by up to 3 constituents.
Given the good coverage achieved by our sample dataset (figure ref:fig:coverage) and according to the scores reported in Table ref:tbl:LoTscores, the RFR model is capable of predicting band gaps at the experimental fidelity with a 0.15 RMSE.
These predictions were projected on the sample space in Figure ref:fig:pred.

#+begin_src jupyter-python :post wrap(*this*, w="450pt", c="label:fig:pred Band gap predictions overlaid on cardinal mixing chemical domain projected from fourteen to two dimensions via t-SNE")
  p = px.scatter(
      projdf,
      facet_col='perplexity', facet_col_wrap=4,
      x='0', y='1',
      hover_name="Formula",
      color="bg_eV",
  )
  p.update_layout(
      coloraxis=dict(colorbar=dict(title="band gap [eV]", title_side='top', orientation='h', )),
      margin=dict(l=0, r=0, t=0, b=20),
  )
  p.update_yaxes(matches=None, visible=False)
  p.update_xaxes(matches=None, visible=False)
  p.show(renderer='svg')
#+end_src

#+RESULTS:
:results:
 
#+attr_latex: :width 450pt
#+CAPTION: label:fig:pred Band gap predictions overlaid on cardinal mixing chemical domain projected from fourteen to two dimensions via t-SNE
[[file:./.ob-jupyter/2a6257e7d00092bdcc8f561301ee50cdbb7b8975.svg]]
:end:

I followed a similar high-throughput screening procedure to that laid out in prior works, except this covered a large space of purely hypothetical compounds.
[cite:@yang-2023-high-throug;@mannodi-kanakkithodi-2021-comput-data]
See figure ref:fig:screenops.
Band gaps between 1 and 2 \unit{\electronvolt} were selected as this range is expected to yield the best power conversion efficiency (PCE) in the visible spectrum.
[cite:@yu-2012-ident-poten;@shockley-1961-detail-balan]
Perovskite compounds were selected for their predicted stability by cutting on each of three tolerance factors previously established in chapter 1.
The constituent ratios of the chosen compositions in figure ref:fig:chosenstats may be juxtaposed with figure ref:fig:domainstats.

#+attr_latex: :width 250pt
#+CAPTION: label:fig:screenops Summary of screening operations used to identify candidate compounds
[[file:screening_ops.png]]

#+begin_src jupyter-python :post wrap(*this*, w="300pt :options inkscapeformat=png, inkscapedpi=300", c="label:fig:chosenstats The compounds selected from the cardinal mixing sample space contain varying fractions of each element")
  p = px.sunburst(XX_big.iloc[screen.index,1:14].replace(0,np.NaN)
                    .join(Y_big[['mix']])
                    .groupby(['mix'])
                    .sum().reset_index().melt(id_vars=['mix']),
                  path=['variable'], values='value')
  p.update_traces(insidetextorientation='horizontal',
                  textinfo="label+percent parent")

  p.update_layout(
      margin=dict(l=0, r=0, t=0, b=0),
      font_size=30
  )
  llist = p.data[0].labels
  p.data[0].labels = [re.sub("comp__|['\(\),)]", "", l) for l in llist]
  p.show(renderer='svg')
#+end_src

#+RESULTS:
:results:
 
#+attr_latex: :width 300pt :options inkscapeformat=png, inkscapedpi=300
#+CAPTION: label:fig:chosenstats The compounds selected from the cardinal mixing sample space contain varying fractions of each element
[[file:./.ob-jupyter/aa9e831e252cb3559416e458c47f5cce2ca96a82.svg]]
:end:

These cuts trimmed the 37785 points by 97% to a subset of only 1251 viable candidates.
These selected candidates were projected onto the t-SNE embedding space in Figure ref:fig:chosen.
A Frequency analysis revealed the constituent elements of the chosen subset most often occupied either small or large shares of their site.
Most A-site constituents preferred occupying 1/8^{th} of their site at a rate of about 8%, with Potassium and Rb also preferring full occupancy 10-12% of the time.
B-site constituents favored pure configurations at a rate of 5-8% but also showed some preference for doping configurations.
X-site constituents, however showed very strong preference for fully occupying their site 25% of the time.
See figure ref:fig:freq.
The strong preference for pure sites simply reflects that this sample space contained compositions mixed at no more than one site simultaneously.

#+begin_src jupyter-python
  dist = X_big.iloc[:,0:14].replace(
      0, np.NaN
  ).reindex(
      screen.index
  ).melt().dropna()

  dist[['site', 'Element']] = dist['variable'].str.replace(
      r"[\(\)']", "", regex=True
  ).str.split(
      ",", 1, expand=True
  )
#+end_src

#+RESULTS:
:results:
:end:

#+begin_src jupyter-python
  def make_percent(df):
      df = df.assign(variable = df.variable / df.variable.sum() * 100)
      return df
#+end_src

#+RESULTS:
:results:
:end:

#+begin_src jupyter-python :post wrap(*this*, w="450pt", c="label:fig:freq Frequency of mixing fractions of species at the A, B, and X sites across the ~1200 screen compounds")
  freq = dist.groupby(['value', 'site', 'Element']).count().reset_index().sort_values('site')

  p = make_subplots(cols=freq.site.unique().size, rows=1,
                    subplot_titles=("A-site", "B-site", "X-site"),
                    shared_xaxes=True,)

  elnum = {'A': 5, 'B': 6, 'X': 3}

  for i, g in enumerate(freq.site.unique()):
      group = freq[freq.site == g].pipe(make_percent)
      colors = dict(zip(group['Element'].unique(), px.colors.qualitative.Plotly))
      traces = []
      for count, (element, sub) in enumerate(group.groupby('Element')):
          color_map = colors[element]
          traces.append(
              go.Bar(
                  x=sub['value'], y=sub['variable'], marker=dict(color=color_map),
                  name=(element if count < elnum[g]-1 else element + '                                                                       '),
                  legendgroup=g, #legendgrouptitle_text=g+'-site',
                  width=0.02
              )
          )
      for trace in traces:
          p.add_trace(trace, col=i+1, row=1)

      p.update_xaxes(
          title='Share of ' + g + ' Site',
          title_font_size=30,
          tickfont_size=24,
          tickmode='array',
          tickvals=np.linspace(0,1,9),
          ticktext=[f'{f.as_integer_ratio()[0]}/{f.as_integer_ratio()[1]}' for f in np.linspace(0,1,9)],
          col=i+1, row=1
      )
      p.update_yaxes(title="Element Frequency (%)",
                     title_font_size=30,
                     tickfont_size=24,
                     title_standoff=0)

  p.for_each_annotation(lambda a: a.update(font_size=30))

  p.update_layout(
      legend=dict(
          bgcolor='rgba(0,0,0,0)',
          orientation="h",
          xanchor='center',
          y=1.0, x=0.62,
          itemsizing='constant',
          font_size=20
      ),
      width=1500, 
  )
  p.show(renderer='svg')
#+end_src

#+RESULTS:
:results:
 
#+attr_latex: :width 450pt
#+CAPTION: label:fig:freq Frequency of mixing fractions of species at the A, B, and X sites across the ~1200 screen compounds
[[file:./.ob-jupyter/09b87051881c9559cbbc102bf3a024c6ebea0f57.svg]]
:end:

* CONCLUSIONS
:PROPERTIES:
:CUSTOM_ID: conclusions
:END:

A set of promising hypothetical compositions is identified by this work.
I identified a set of 1251 promising perovskite compositions with desirable band gaps for Photovoltaic applications using a novel data-driven approach.
The selected band gaps are predicted at an experimental fidelity with error likely ranging from 0.15 to 0.3 \unit{\electronvolt}.
Of the selected compositions 640 are purely inorganic, and 611 are hybrid-organic/inorganic.
Table ref:tbl:mixscreen shows how the set subdivides by mixing.
Only 40 of the original expertly designed sample pass the screening, the rest are untested to my knowledge.
This shows that there is still much opportunity for discovery in this area and much to be learned about this chemistry.
Notably, 834 contain no lead.
The 30 most stable lead-free compounds identified with band-gap in the 1-2 \unit{\electronvolt} range are listed in table ref:tbl:pbfree.

I evaluated a variety of machine learning techniques and implemented simple but effective models for estimating band gap for perovskites from multi-fidelity data.
# On held-out test data, a random forest model achieved nearly an RMSE error of 0.12 on the whole dataset and 0.15 on the experimental subset.
# This compared favorably to prior element property based models of band gap reported by [cite/text:@pilania-2016-machin-learn] for double perovskites.
# The models trained here used more that three times the descriptors of prior models and had access to much more data.
I found RFR models performed best in this setting, and believe this is because the architecture is an ensemble model so it reduces the variance of error by design.
Additionally, the decision trees that make up the forest best identify and capture relevant feature interactions as corroborated by Pearson correlations.
Trees also flexibly represent complex nonlinear relationships between feature interactions and band gap.
RFR models may be data hungry but are generally better about accommodating outliers.
The RFR properties that helped it perform best in this setting are not domain-specific and therefore are likely to apply to other material properties and for other types of compounds.

In summary, I experimented with training multiple models to differentiate and treat appropriately observations with different fidelities and demonstrated good results with the RFR in particular.
The Gaussian Process Regression was a very close second.
The SISSO model and the RFR and GPR based on the SIS-selected superspace do not perform well, contrary to expectations.
The GPRs ability to theoretically deal with more complex descriptors deserves further study to realize its full potential.
Further improving on the feature engineering and examining what can be learned about the relationship between structure and band gap might begin here.

#+begin_src jupyter-python :post wraptbl(c="label:tbl:mixscreen Number of selected data points with given mixing site")
  s = screen.mix.value_counts()
  s.name = "count"
  s.to_frame()
#+end_src

#+RESULTS:
:results:
 
#+CAPTION: label:tbl:mixscreen Number of selected data points with given mixing site 
|      | count |
|------+-------|
| A    | 605   |
| B    | 388   |
| X    | 256   |
| pure | 2     |
:end:

#+begin_src jupyter-python :post wraptbl(c="label:tbl:pbfree Thirty hypothetical lead-free formulae and their predicted band gaps")
  randdf = screen[~screen.Formula.str.contains(r"Pb", regex=False)].sort_values("DecoE_eV", ascending=True).head(30)
  randdf = randdf[["Formula", "bg_eV"]].rename(columns={"bg_eV":"band gap [eV]"})
  randdf
#+end_src

#+RESULTS:
:results:
 
#+CAPTION: label:tbl:pbfree Thirty hypothetical lead-free formulae and their predicted band gaps 
|       | Formula                                   | band gap [eV] |
|-------+-------------------------------------------+---------------|
| 19290 | FA0.375Rb0.625Sn1.000I1.000               |          1.98 |
| 19309 | FA0.375MA0.125Rb0.500Sn1.000Cl1.000       |          1.99 |
| 19310 | FA0.375MA0.125Rb0.500Sn1.000Br1.000       |          1.95 |
| 19306 | FA0.375MA0.125Rb0.500Sr1.000Cl1.000       |          1.95 |
| 19308 | FA0.375MA0.125Rb0.500Sn1.000I1.000        |          1.70 |
| 19307 | FA0.375MA0.125Rb0.500Sr1.000Br1.000       |          1.96 |
| 19304 | FA0.375Rb0.625Ba1.000Br1.000              |          1.93 |
| 19303 | FA0.375Rb0.625Ba1.000Cl1.000              |          1.89 |
| 19305 | FA0.375MA0.125Rb0.500Sr1.000I1.000        |          1.74 |
| 19302 | FA0.375Rb0.625Ba1.000I1.000               |          1.68 |
| 19076 | FA0.250K0.250MA0.375Rb0.125Sn1.000Br1.000 |          1.93 |
| 19301 | FA0.375Rb0.625Ca1.000Br1.000              |          1.69 |
| 19300 | FA0.375Rb0.625Ca1.000Cl1.000              |          1.72 |
| 19324 | FA0.375MA0.250Rb0.375Sr1.000Cl1.000       |          1.72 |
| 19008 | FA0.250K0.125MA0.625Ge1.000I1.000         |          1.93 |
| 19056 | FA0.250K0.250MA0.250Rb0.250Sn1.000I1.000  |          1.84 |
| 19004 | FA0.250K0.125MA0.625Sn1.000Br1.000        |          1.77 |
| 19032 | FA0.250K0.250Rb0.500Ba1.000I1.000         |          1.95 |
| 19073 | FA0.250K0.250MA0.375Rb0.125Sr1.000Br1.000 |          1.94 |
| 19094 | FA0.250K0.250MA0.500Sn1.000Br1.000        |          1.87 |
| 19049 | FA0.250K0.250MA0.125Rb0.375Ca1.000Br1.000 |          1.85 |
| 19003 | FA0.250K0.125MA0.625Sn1.000Cl1.000        |          1.75 |
| 19074 | FA0.250K0.250MA0.375Rb0.125Sn1.000I1.000  |          1.68 |
| 19002 | FA0.250K0.125MA0.625Sn1.000I1.000         |          1.46 |
| 19053 | FA0.250K0.250MA0.250Rb0.250Sr1.000I1.000  |          1.88 |
| 19075 | FA0.250K0.250MA0.375Rb0.125Sn1.000Cl1.000 |          1.95 |
| 19070 | FA0.250K0.250MA0.250Rb0.250Ba1.000Br1.000 |          1.86 |
| 19093 | FA0.250K0.250MA0.500Sn1.000Cl1.000        |          1.86 |
| 19001 | FA0.250K0.125MA0.625Sr1.000Br1.000        |          1.71 |
| 19091 | FA0.250K0.250MA0.500Sr1.000Br1.000        |          1.89 |
:end:

* Footnotes

[fn:10]https://ai-materials-and-chemistry.gitbook.io/foundry/
[fn:9]https://github.com/slundberg/shap
[fn:8]https://github.com/PanayotisManganaris/manuscript--multifidelity-dft-ml.git 
[fn:7]amannodi@purdue.edu
[fn:6]https://github.com/Matgenix/pysisso 
[fn:5]https://github.com/PanayotisManganaris/pysisso
[fn:4]https://github.com/PanayotisManganaris/yogi 
[fn:3]https://github.com/PanayotisManganaris/cmcl
[fn:2]https://cmr.fysik.dtu.dk/ 
[fn:1]https://github.com/rouyang2017/SISSO 

#+NAME: wrap
#+begin_src bash :var p="" :var w="300pt" :var c=""
  echo -ne "$p \n#+attr_latex: :width $w\n#+CAPTION: $c"
#+end_src

#+NAME: wraptbl
#+begin_src bash :var p="" :var w="300pt" :var c=""
  echo -ne "$p \n#+CAPTION: $c "
#+end_src

#+name: glossary
| label | term                    | definition                                                                                                                                                                                                                              |
|-------+-------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| cmix  | cardinal mixing         | Describes perovskite alloys where no more than one of the A, B, or X sites is occupied by multiple possible constituents                                                                                                                |
| prtn  | partition               | Portion of sample data reserved for a purpose in model development                                                                                                                                                                      |
| cv    | cross-validation        | Method for gathering statistics on the abilities of a model to fit to the parent partition                                                                                                                                              |
| kfs   | K-fold split            | Data partition divided into K arbitrary groups for use in cross-validation schemes                                                                                                                                                      |
| gkf   | groupwise K-fold        | Data partition divided into K-folds where each fold corresponds to a category label                                                                                                                                                     |
| lot   | level of theory         | Refers to the rank of a DFT functional in the hierarchy of phenomenological comprehensiveness. A proxy for accuracy.                                                                                                                 |
| mp    | Materials Project       | US Government-led multidisciplinary collaboration founded in 2011 as the Materials Genome Initiative.                                                                                                                                   |
| ml    | machine learning        | a science concerned with algorithms which improve their performance with exposure to new data                                                                                                                                           |
| ft    | features                | attributes of an observed event or object which might empirically explain the event or object                                                                                                                                           |
| hp    | hyper-parameter         | a setting that controls how a learning algorithm works                                                                                                                                                                                  |
| cl    | classical learning      | a paradigm of machine learning that is dependent on expert knowledge to extract quality features from samples in a dataset                                                                                                              |
| sm    | surrogate model         | a representation which attempts to capture as much of the relationship between a domain and a target property as possible                                                                                                               |
| dl    | deep learning           | a paradigm of machine learning differing from classical learning in that the features of the input data are themselves learned by the algorithm                                                                                         |
| ls    | latent space            | a multidimensional abstraction of a problem space. the relationship between coordinates in this space and observation in the real world can be formulated to guarantee the viability of solutions in the abstraction                    |
| eva   | evolutionary algorithms | a class of nature-inspire algorithms often applied to optimization in high dimensional discontinuous functions                                                                                                                          |
| agn   | ALIGNN                  | the Atomistic Line Graph Neural Network considers relative positions of atoms in a crystal as well as the relative angles between bonds by creating two related node and edge graphs and convluting them in a staggered manner together |
| gp    | Gaussian Process        | Any function which returns samples from an underlying multivariate normal distribution                                                                                                                                                  |
| fair  | FAIR                    | Findable Accessible Interoperable and Reusable Data                                                                                                                                                                                     |
| mtl   | multi-task learning     | A type of machine learning where an algorithm learns multiple functions simultaneously, while exploiting commonalities and differences between the functions                                                                            |
| soc   | Spin Orbit Coupling     | An additional term intended to account for the increased relevance of quantum angular momentum to electromagnetic response in heavy atoms                                                                                               |

#+name: acronyms
| label | abbreviation | full form                                            |
|-------+--------------+------------------------------------------------------|
| hap   | HaP          | halide perovskite                                    |
| vasp  | VASP         | Vienna Ab initio Simulation Package                  |
| qmml  | QM/ML        | quantum mechanics machine learning                   |
| slme  | SLME         | spectroscopic limited maximum efficiency             |
| pce   | PCE          | power conversion efficiency                          |
| dft   | DFT          | density functional theory                            |
| gga   | GGA          | generalized gradient approximation                   |
| pbe   | PBE          | Perdew-Burke-Ernzerhof Functional                    |
| hse   | HSE06        | Heyd-Scuseria-Ernzerhof Functional                   |
| ma    | MA           | Methylammonium                                       |
| fa    | FA           | Formamidinium                                        |
| pca   | PCA          | principal component analysis                         |
| tsne  | t-SNE        | t-distributed stochastic neighbor embedding          |
| umap  | UMAP         | uniform manifold approximation and projection        |
| gpr   | GPR          | Gaussian Process Regression                          |
| rfr   | RFR          | Random Forest Regression                             |
| sisso | SISSO        | Sure Independence Screening and Sparsifying Operator |
| sqs   | SQS          | special quasi-random structures                      |
| paw   | PAW          | projector augmented wave                             |
| nist  | NIST         | National Institute of Standards and Technology       |
| pes   | PES          | Potential Energy Surface                             |
| shap  | SHAP         | Shapley Additive Explaination                        |
| gnn   | GNN          | Graph Neural Networks                                |


